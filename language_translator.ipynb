{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12892938,"sourceType":"datasetVersion","datasetId":8157298}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install indic-nlp-library","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:46:44.680989Z","iopub.execute_input":"2025-08-28T09:46:44.683222Z","iopub.status.idle":"2025-08-28T09:46:49.088810Z","shell.execute_reply.started":"2025-08-28T09:46:44.683179Z","shell.execute_reply":"2025-08-28T09:46:49.087590Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.11/dist-packages (0.92)\nRequirement already satisfied: sphinx-argparse in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.5.2)\nRequirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.2.4)\nRequirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.6)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\nRequirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\nRequirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\nRequirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\nRequirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.4)\nRequirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\nRequirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->indic-nlp-library) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->indic-nlp-library) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.6.15)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd  #for dataset\n\nimport numpy as np   # for matrices and array\n\nfrom transformers import AutoTokenizer # trnasformer is a library and AutoTokenizer is a part of it.\n# Tokenizer => To break a sentence into words is called tokenization.It is done so that the data can be easily understood by the model.\n# Ex : I am with 5th Sem students of KIET. => AFter tokenization : [I,am,with,5th,Sem,students,of,KIET]\n# Natural language data is the hardest to train\n# Autotokenizer is a program which as great functions to tokenize such type of hard data\n# Tokenizers are also trained ml models to do tokenization\n\nimport torch \nfrom indicnlp.tokenize import indic_tokenize\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:47:17.338563Z","iopub.execute_input":"2025-08-28T09:47:17.339128Z","iopub.status.idle":"2025-08-28T09:47:17.344502Z","shell.execute_reply.started":"2025-08-28T09:47:17.339100Z","shell.execute_reply":"2025-08-28T09:47:17.343374Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/english-hindi/Sentence pairs in English-Hindi - 2025-02-11.tsv\",\n                  sep=\"\\t\", header=None, names=[\"SrcSentID\",\"SrcSent\",\"DstSentID\",\"DstSent\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:47:21.272967Z","iopub.execute_input":"2025-08-28T09:47:21.273757Z","iopub.status.idle":"2025-08-28T09:47:21.338879Z","shell.execute_reply.started":"2025-08-28T09:47:21.273729Z","shell.execute_reply":"2025-08-28T09:47:21.338066Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data.drop(labels=[data.columns[0], data.columns[2]], axis=1, inplace=True)\n# This line removes the first and third columns of the dataframe, permanently updating data.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:48:15.597883Z","iopub.execute_input":"2025-08-28T09:48:15.598240Z","iopub.status.idle":"2025-08-28T09:48:15.605157Z","shell.execute_reply.started":"2025-08-28T09:48:15.598216Z","shell.execute_reply":"2025-08-28T09:48:15.604099Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:48:21.826494Z","iopub.execute_input":"2025-08-28T09:48:21.826887Z","iopub.status.idle":"2025-08-28T09:48:21.836384Z","shell.execute_reply.started":"2025-08-28T09:48:21.826857Z","shell.execute_reply":"2025-08-28T09:48:21.835497Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                   SrcSent  \\\n0                       Muiriel is 20 now.   \n1                       Muiriel is 20 now.   \n2  Education in this world disappoints me.   \n3                       That won't happen.   \n4                              I miss you.   \n\n                                       DstSent  \n0             म्यूरियल अब बीस साल की हो गई है।  \n1                   म्यूरियल अब बीस साल की है।  \n2  मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।  \n3                              वैसा नहीं होगा।  \n4                 मुझें तुम्हारी याद आ रही है।  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SrcSent</th>\n      <th>DstSent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Muiriel is 20 now.</td>\n      <td>म्यूरियल अब बीस साल की हो गई है।</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Muiriel is 20 now.</td>\n      <td>म्यूरियल अब बीस साल की है।</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Education in this world disappoints me.</td>\n      <td>मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>That won't happen.</td>\n      <td>वैसा नहीं होगा।</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I miss you.</td>\n      <td>मुझें तुम्हारी याद आ रही है।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# to tokenize english sentence we will use AutoTokenizer \n# for hindi => indic_tokenize\nsrc_sent_tokenizer = AutoTokenizer.from_pretrained(\"google-T5/T5-base\") # T5 is an LLM based on tenasformers made by google\n# we are using base model of T5\n# \"google-T5/T5-base\" => It is like a path to a repository on internet => hugging face ki ek repository hogi uske andr ggoogle-T5 ek repository hogi jiske andr ek folder hoga T5-base\n# hr LLM ka Apna ek tokenizer hota hai jo ki ek alg trh se train kiya jata hai\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:48:42.446538Z","iopub.execute_input":"2025-08-28T09:48:42.446942Z","iopub.status.idle":"2025-08-28T09:48:43.664179Z","shell.execute_reply.started":"2025-08-28T09:48:42.446914Z","shell.execute_reply":"2025-08-28T09:48:43.662782Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data[\"SrcSent\"] = data[\"SrcSent\"].apply(lambda x : src_sent_tokenizer.tokenize(x))\n# hr cell pr kch operation krna hai to applymap use krenge\n# kisi ek row ya column  pr opr krna hai to yse apply\n# agr hashing krni hai to t=use map\n# panda has => apply, map, applymap\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:49:14.696496Z","iopub.execute_input":"2025-08-28T09:49:14.697350Z","iopub.status.idle":"2025-08-28T09:49:15.786702Z","shell.execute_reply.started":"2025-08-28T09:49:14.697306Z","shell.execute_reply":"2025-08-28T09:49:15.785763Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:49:20.846793Z","iopub.execute_input":"2025-08-28T09:49:20.847121Z","iopub.status.idle":"2025-08-28T09:49:20.858026Z","shell.execute_reply.started":"2025-08-28T09:49:20.847096Z","shell.execute_reply":"2025-08-28T09:49:20.856956Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                             SrcSent  \\\n0                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n1                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n2  [▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...   \n3                    [▁That, ▁won, ', t, ▁happen, .]   \n4                               [▁I, ▁miss, ▁you, .]   \n\n                                       DstSent  \n0             म्यूरियल अब बीस साल की हो गई है।  \n1                   म्यूरियल अब बीस साल की है।  \n2  मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।  \n3                              वैसा नहीं होगा।  \n4                 मुझें तुम्हारी याद आ रही है।  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SrcSent</th>\n      <th>DstSent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n      <td>म्यूरियल अब बीस साल की हो गई है।</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n      <td>म्यूरियल अब बीस साल की है।</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...</td>\n      <td>मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[▁That, ▁won, ', t, ▁happen, .]</td>\n      <td>वैसा नहीं होगा।</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[▁I, ▁miss, ▁you, .]</td>\n      <td>मुझें तुम्हारी याद आ रही है।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# this type of tokenization is called sub-word tokenization as it also breaks a single word into parts and it also treart space as a token represnted by unserscore(_)\n# BPE(Byte-Pair encoding) => Algorithm is used behind this to do this type of tokenization\n# WPT(Word Piece Tokenizer) => it is also an algo for tokenization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:38:59.401734Z","iopub.execute_input":"2025-08-28T09:38:59.402053Z","iopub.status.idle":"2025-08-28T09:38:59.419830Z","shell.execute_reply.started":"2025-08-28T09:38:59.402032Z","shell.execute_reply":"2025-08-28T09:38:59.418824Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"data[\"DstSent\"] = data[\"DstSent\"].apply(lambda x : indic_tokenize.trivial_tokenize(x,lang=\"hi\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:49:45.382136Z","iopub.execute_input":"2025-08-28T09:49:45.382455Z","iopub.status.idle":"2025-08-28T09:49:45.540569Z","shell.execute_reply.started":"2025-08-28T09:49:45.382431Z","shell.execute_reply":"2025-08-28T09:49:45.539608Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:49:51.844847Z","iopub.execute_input":"2025-08-28T09:49:51.845214Z","iopub.status.idle":"2025-08-28T09:49:51.856643Z","shell.execute_reply.started":"2025-08-28T09:49:51.845189Z","shell.execute_reply":"2025-08-28T09:49:51.855494Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                             SrcSent  \\\n0                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n1                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n2  [▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...   \n3                    [▁That, ▁won, ', t, ▁happen, .]   \n4                               [▁I, ▁miss, ▁you, .]   \n\n                                             DstSent  \n0        [म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]  \n1                [म्यूरियल, अब, बीस, साल, की, है, ।]  \n2  [मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...  \n3                              [वैसा, नहीं, होगा, ।]  \n4              [मुझें, तुम्हारी, याद, आ, रही, है, ।]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SrcSent</th>\n      <th>DstSent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n      <td>[म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n      <td>[म्यूरियल, अब, बीस, साल, की, है, ।]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...</td>\n      <td>[मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[▁That, ▁won, ', t, ▁happen, .]</td>\n      <td>[वैसा, नहीं, होगा, ।]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[▁I, ▁miss, ▁you, .]</td>\n      <td>[मुझें, तुम्हारी, याद, आ, रही, है, ।]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"data[\"SrcSent\"] = data[\"SrcSent\"].apply(src_sent_tokenizer.convert_tokens_to_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:50:18.729178Z","iopub.execute_input":"2025-08-28T09:50:18.729560Z","iopub.status.idle":"2025-08-28T09:50:18.819703Z","shell.execute_reply.started":"2025-08-28T09:50:18.729531Z","shell.execute_reply":"2025-08-28T09:50:18.818654Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"Vs = src_sent_tokenizer.get_vocab() # hr src word ko ek number assign kr dega","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:38:59.716750Z","iopub.execute_input":"2025-08-28T09:38:59.717012Z","iopub.status.idle":"2025-08-28T09:38:59.763889Z","shell.execute_reply.started":"2025-08-28T09:38:59.716992Z","shell.execute_reply":"2025-08-28T09:38:59.762379Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:50:54.988871Z","iopub.execute_input":"2025-08-28T09:50:54.989330Z","iopub.status.idle":"2025-08-28T09:50:55.000966Z","shell.execute_reply.started":"2025-08-28T09:50:54.989297Z","shell.execute_reply":"2025-08-28T09:50:55.000008Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                 SrcSent  \\\n0     [4159, 23, 14018, 19, 460, 230, 5]   \n1     [4159, 23, 14018, 19, 460, 230, 5]   \n2  [2855, 16, 48, 296, 26963, 7, 140, 5]   \n3            [466, 751, 31, 17, 1837, 5]   \n4                      [27, 3041, 25, 5]   \n\n                                             DstSent  \n0        [म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]  \n1                [म्यूरियल, अब, बीस, साल, की, है, ।]  \n2  [मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...  \n3                              [वैसा, नहीं, होगा, ।]  \n4              [मुझें, तुम्हारी, याद, आ, रही, है, ।]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SrcSent</th>\n      <th>DstSent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n      <td>[म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n      <td>[म्यूरियल, अब, बीस, साल, की, है, ।]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[2855, 16, 48, 296, 26963, 7, 140, 5]</td>\n      <td>[मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[466, 751, 31, 17, 1837, 5]</td>\n      <td>[वैसा, नहीं, होगा, ।]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[27, 3041, 25, 5]</td>\n      <td>[मुझें, तुम्हारी, याद, आ, रही, है, ।]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"hindi_vocab = set()\n\nfor tokenized_hindi_sent in data[\"DstSent\"] :\n    hindi_vocab.update(tokenized_hindi_sent) # this is a set of unique hindi words from our dst column","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:51:15.542118Z","iopub.execute_input":"2025-08-28T09:51:15.542442Z","iopub.status.idle":"2025-08-28T09:51:15.558202Z","shell.execute_reply.started":"2025-08-28T09:51:15.542419Z","shell.execute_reply":"2025-08-28T09:51:15.557174Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"Vd = dict()\nfor idx, token in enumerate(hindi_vocab) : \n    Vd[token] = idx + 3\nVd[\"<PAD>\"] = 0\nVd[\"<SOS>\"] = 1\nVd[\"<EOS>\"] = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:51:33.921957Z","iopub.execute_input":"2025-08-28T09:51:33.922289Z","iopub.status.idle":"2025-08-28T09:51:33.931952Z","shell.execute_reply.started":"2025-08-28T09:51:33.922265Z","shell.execute_reply":"2025-08-28T09:51:33.930843Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Tokens in neural network: \n\"<SOS>\" (1) => Start of Sentence => USed to denote or tell the neural network that a sentece is going to start.\n\"<PAD>\" (0) => Padding => Used for doing padding to make the length of all the sentence equal, so that a matrix can be made\"\n\"<EOS>\" (2) => End of Service => USed to denote the end of the sentence.","metadata":{}},{"cell_type":"code","source":"def convert_hindi_tokens_to_ids(hindi_sent) : \n    return [Vd[token] for token in hindi_sent]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:51:52.392076Z","iopub.execute_input":"2025-08-28T09:51:52.393173Z","iopub.status.idle":"2025-08-28T09:51:52.398102Z","shell.execute_reply.started":"2025-08-28T09:51:52.393136Z","shell.execute_reply":"2025-08-28T09:51:52.396239Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data[\"DstSent\"] = data[\"DstSent\"].apply(lambda x : convert_hindi_tokens_to_ids(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:52:20.208977Z","iopub.execute_input":"2025-08-28T09:52:20.209754Z","iopub.status.idle":"2025-08-28T09:52:20.242414Z","shell.execute_reply.started":"2025-08-28T09:52:20.209713Z","shell.execute_reply":"2025-08-28T09:52:20.241475Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:52:25.795782Z","iopub.execute_input":"2025-08-28T09:52:25.796143Z","iopub.status.idle":"2025-08-28T09:52:25.807510Z","shell.execute_reply.started":"2025-08-28T09:52:25.796116Z","shell.execute_reply":"2025-08-28T09:52:25.806503Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                 SrcSent  \\\n0     [4159, 23, 14018, 19, 460, 230, 5]   \n1     [4159, 23, 14018, 19, 460, 230, 5]   \n2  [2855, 16, 48, 296, 26963, 7, 140, 5]   \n3            [466, 751, 31, 17, 1837, 5]   \n4                      [27, 3041, 25, 5]   \n\n                                             DstSent  \n0  [6601, 338, 5511, 656, 4176, 921, 4002, 4627, ...  \n1           [6601, 338, 5511, 656, 4176, 4627, 1179]  \n2  [3062, 3520, 279, 758, 6082, 5797, 5501, 1054,...  \n3                             [380, 1922, 568, 1179]  \n4         [1323, 5957, 2601, 2048, 5702, 4627, 1179]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SrcSent</th>\n      <th>DstSent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n      <td>[6601, 338, 5511, 656, 4176, 921, 4002, 4627, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n      <td>[6601, 338, 5511, 656, 4176, 4627, 1179]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[2855, 16, 48, 296, 26963, 7, 140, 5]</td>\n      <td>[3062, 3520, 279, 758, 6082, 5797, 5501, 1054,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[466, 751, 31, 17, 1837, 5]</td>\n      <td>[380, 1922, 568, 1179]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[27, 3041, 25, 5]</td>\n      <td>[1323, 5957, 2601, 2048, 5702, 4627, 1179]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# adding SOS(1) in start\ndef insert_sos_token_id(hindi_sent_token_ids):\n    return [1] + hindi_sent_token_ids # Concatenation hora h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:52:52.460584Z","iopub.execute_input":"2025-08-28T09:52:52.460991Z","iopub.status.idle":"2025-08-28T09:52:52.466054Z","shell.execute_reply.started":"2025-08-28T09:52:52.460965Z","shell.execute_reply":"2025-08-28T09:52:52.464933Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"data[\"DstSentInput\"] = data[\"DstSent\"].apply(lambda x : insert_sos_token_id(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:53:17.405229Z","iopub.execute_input":"2025-08-28T09:53:17.405602Z","iopub.status.idle":"2025-08-28T09:53:17.421219Z","shell.execute_reply.started":"2025-08-28T09:53:17.405574Z","shell.execute_reply":"2025-08-28T09:53:17.420180Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# adding EOS (2) at end\ndef insert_eos_token_id(hindi_sent_token_ids): \n    return hindi_sent_token_ids + [2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:54:20.198913Z","iopub.execute_input":"2025-08-28T09:54:20.199234Z","iopub.status.idle":"2025-08-28T09:54:20.203705Z","shell.execute_reply.started":"2025-08-28T09:54:20.199213Z","shell.execute_reply":"2025-08-28T09:54:20.202558Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"data[\"DstSentLabel\"] = data[\"DstSent\"].apply(lambda x : insert_eos_token_id(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:54:24.828648Z","iopub.execute_input":"2025-08-28T09:54:24.829075Z","iopub.status.idle":"2025-08-28T09:54:24.844485Z","shell.execute_reply.started":"2025-08-28T09:54:24.829049Z","shell.execute_reply":"2025-08-28T09:54:24.843091Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"data.drop(labels=[data.columns[1]],axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:54:38.348706Z","iopub.execute_input":"2025-08-28T09:54:38.349060Z","iopub.status.idle":"2025-08-28T09:54:38.356738Z","shell.execute_reply.started":"2025-08-28T09:54:38.349028Z","shell.execute_reply":"2025-08-28T09:54:38.355866Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"X = list(data[\"SrcSent\"])\nY_input = list(data[\"DstSentInput\"])\nY_label = list(data[\"DstSentLabel\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:55:17.177561Z","iopub.execute_input":"2025-08-28T09:55:17.178172Z","iopub.status.idle":"2025-08-28T09:55:17.193964Z","shell.execute_reply.started":"2025-08-28T09:55:17.178133Z","shell.execute_reply":"2025-08-28T09:55:17.191554Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"X_tensor = [torch.tensor(tokenized_eng_sent_ids) for tokenized_eng_sent_ids in X]\nY_input_tensor = [torch.tensor(tokenized_hin_sent_ids) for tokenized_hin_sent_ids in Y_input]\nY_label_tensor = [torch.tensor(tokenized_hin_sent_ids) for tokenized_hin_sent_ids in Y_label]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:56:57.584106Z","iopub.execute_input":"2025-08-28T09:56:57.585037Z","iopub.status.idle":"2025-08-28T09:56:57.937298Z","shell.execute_reply.started":"2025-08-28T09:56:57.584993Z","shell.execute_reply":"2025-08-28T09:56:57.936040Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"X_padded = torch.nn.utils.rnn.pad_sequence(X_tensor,batch_first=True)\nY_padded_input = torch.nn.utils.rnn.pad_sequence(Y_input_tensor,batch_first=True)\nY_padded_label = torch.nn.utils.rnn.pad_sequence(Y_label_tensor,batch_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:57:35.629939Z","iopub.execute_input":"2025-08-28T09:57:35.630833Z","iopub.status.idle":"2025-08-28T09:57:35.858061Z","shell.execute_reply.started":"2025-08-28T09:57:35.630792Z","shell.execute_reply":"2025-08-28T09:57:35.857064Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"Ns = X_padded.shape[1]\nNd = Y_padded_label.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T09:57:46.712493Z","iopub.execute_input":"2025-08-28T09:57:46.712851Z","iopub.status.idle":"2025-08-28T09:57:46.717485Z","shell.execute_reply.started":"2025-08-28T09:57:46.712826Z","shell.execute_reply":"2025-08-28T09:57:46.716507Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}